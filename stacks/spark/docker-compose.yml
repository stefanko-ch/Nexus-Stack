# =============================================================================
# Apache Spark - Distributed Data Processing Engine
# =============================================================================
# Access via: https://spark.${domain} (Master Web UI)
# Apache Spark provides a unified analytics engine for large-scale data
# processing. This stack runs a standalone cluster with one master and one
# worker node.
#
# Jupyter PySpark connects to this cluster via spark://spark-master:7077
# S3 access (Hetzner Object Storage) is pre-configured via Hadoop properties.
#
# SECURITY:
# - Protected by Cloudflare Access (email OTP authentication)
# - No authentication on Spark itself (relies on Cloudflare Access)
# =============================================================================

services:
  spark-master:
    image: ${IMAGE_SPARK:-apache/spark:4.1.1}
    container_name: spark-master
    hostname: spark-master
    restart: unless-stopped
    environment:
      SPARK_MODE: master
      SPARK_MASTER_HOST: spark-master
      SPARK_MASTER_PORT: "7077"
      SPARK_MASTER_WEBUI_PORT: "8080"
      # S3 (Hetzner Object Storage) configuration via Hadoop properties
      SPARK_HADOOP_fs_s3a_endpoint: ${HETZNER_S3_ENDPOINT:-}
      SPARK_HADOOP_fs_s3a_access_key: ${HETZNER_S3_ACCESS_KEY:-}
      SPARK_HADOOP_fs_s3a_secret_key: ${HETZNER_S3_SECRET_KEY:-}
      SPARK_HADOOP_fs_s3a_path_style_access: "true"
      SPARK_HADOOP_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
    env_file:
      - path: .env
        required: false
    entrypoint: ["/bin/bash", "-c"]
    command:
      - |
        /opt/spark/sbin/start-master.sh --host spark-master --port 7077 --webui-port 8080
        tail -f /opt/spark/logs/spark--org.apache.spark.deploy.master.Master-*.out 2>/dev/null || tail -f /dev/null
    ports:
      - "8088:8080"
    volumes:
      - spark-master-logs:/opt/spark/logs
    networks:
      - app-network
      - spark-internal
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/ || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 20s

  spark-worker:
    image: ${IMAGE_SPARK_WORKER:-apache/spark:4.1.1}
    container_name: spark-worker
    hostname: spark-worker
    restart: unless-stopped
    depends_on:
      spark-master:
        condition: service_healthy
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_CORES: ${SPARK_WORKER_CORES:-2}
      SPARK_WORKER_MEMORY: ${SPARK_WORKER_MEMORY:-2g}
      # S3 (Hetzner Object Storage) configuration
      SPARK_HADOOP_fs_s3a_endpoint: ${HETZNER_S3_ENDPOINT:-}
      SPARK_HADOOP_fs_s3a_access_key: ${HETZNER_S3_ACCESS_KEY:-}
      SPARK_HADOOP_fs_s3a_secret_key: ${HETZNER_S3_SECRET_KEY:-}
      SPARK_HADOOP_fs_s3a_path_style_access: "true"
      SPARK_HADOOP_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
    env_file:
      - path: .env
        required: false
    entrypoint: ["/bin/bash", "-c"]
    command:
      - |
        /opt/spark/sbin/start-worker.sh spark://spark-master:7077
        tail -f /opt/spark/logs/spark--org.apache.spark.deploy.worker.Worker-*.out 2>/dev/null || tail -f /dev/null
    volumes:
      - spark-worker-data:/opt/spark/work
      - spark-worker-logs:/opt/spark/logs
    networks:
      - app-network
      - spark-internal

volumes:
  spark-master-logs:
  spark-worker-data:
  spark-worker-logs:

networks:
  app-network:
    external: true
  spark-internal:
    driver: bridge
